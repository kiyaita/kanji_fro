＃１元ネタこれです↓
https://www.youtube.com/watch?v=up0K74Tz10Q&loop=0

＃２環境構築
anacondaの環境データがkanji_fro.ymlに入ってるので、
conda env create -f kanji_fro.yml
から復元してください。
https://helve-blog.com/posts/python/conda-virtual-environment/

＃３workspace
workspaceの名前のディレクトリは適当に動作試してただけなので、消していいです。
_checkpointのpythonファイルもgitを使いこなせなかった人の苦しいバックアップです。

＃４ディレクトリ構造
現段階で本体にかかわる必要なファイル（フォルダ）は以下の３つのみです。それ以外は＃３の通り、メモに置いてあるだけです。

handpose+video.py   (本体)
video_concat_dir    (まだ使ってない。両手合わせた時に表示される動画の予定)
video_dir           (漢字のそれぞれの部首のパーツを表示する予定の動画の予定。片手ずつそれぞれ、偏とつくりを映す)

＃５進捗
軽く今までの進捗

googleの作ったらしい、人物推定ライブラリのmediapipeを使って、手を認識しています。
medipipeからは、手の平の範囲のbboxの座標情報と、右手か左手かの情報をとってきています。
medipipeは認識のみ行うため、動画のフレームを超えた物体追跡はしません。

物体追跡はmotpyを使用しています。motpyはカルマンフィルターを使ってるらしいけど、詳しいことは不知火。
medipipeから得たbboxの座標から、フレーム間で同じ手かどうかを判別してidをつける。

あとはidに対応して動画をbboxに合うように張り付けて、一定距離二つの手が近づいたらくっつけて動画再生　＜ーー（いまここの途中）
一定距離近づいた手同志を緑の線でつないだとこが最新。
「一定距離離れたらidのペアを外す」を作りたい。一定距離離れたら、動画とidリセットで問題ないかも。

なんか両手移した後、片手だけフェードアウトしたときの、動画の挙動がおかしいのも修正しなければ、、、


